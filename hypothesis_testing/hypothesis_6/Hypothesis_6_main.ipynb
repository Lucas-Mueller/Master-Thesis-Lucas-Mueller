{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis 6 — Main Notebook\n",
    "\n",
    "This notebook prepares three sets of 34 experiment configurations (English, Spanish, Mandarin),\n",
    "runs them selectively in parallel, and compares outcome distributions across languages.\n",
    "\n",
    "Requirements implemented:\n",
    "- 34 configs per language (total 102) with agent temperatures drawn from U(0, 1.5) per config.\n",
    "- Separate subfolders per language under `configs/`, `terminal_outputs/`, and `results/`.\n",
    "- Statistical analysis comparing the outcomes across the three language sets (5×3 table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3f40f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure repo root on sys.path (for local package imports)\n",
    "def _add_repo_root_to_sys_path():\n",
    "    here = Path.cwd().resolve()\n",
    "    for p in [here] + list(here.parents):\n",
    "        if (p / 'main.py').exists() and (p / 'hypothesis_testing').is_dir():\n",
    "            if str(p) not in sys.path:\n",
    "                sys.path.insert(0, str(p))\n",
    "            return p\n",
    "    return here\n",
    "_REPO_ROOT = _add_repo_root_to_sys_path()\n",
    "\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import yaml\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from hypothesis_testing.utils_hypothesis_testing.runner import (\n",
    "    list_config_files,\n",
    "    select_configs,\n",
    "    run_configs_in_parallel,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d44a1f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/Users/lucasmuller/Desktop/Githubg/Rawls_v3/hypothesis_testing/hypothesis_6/configs'),\n",
       " PosixPath('/Users/lucasmuller/Desktop/Githubg/Rawls_v3/hypothesis_testing/hypothesis_6/terminal_outputs'),\n",
       " PosixPath('/Users/lucasmuller/Desktop/Githubg/Rawls_v3/hypothesis_testing/hypothesis_6/results'),\n",
       " PosixPath('/Users/lucasmuller/Desktop/Githubg/Rawls_v3/hypothesis_testing/hypothesis_6/transcripts'),\n",
       " {'english': 'English', 'spanish': 'Spanish', 'mandarin': 'Mandarin'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base paths and per-language subfolders\n",
    "BASE_DIR = _REPO_ROOT / 'hypothesis_testing' / 'hypothesis_6'\n",
    "CONFIGS_BASE = BASE_DIR / 'configs'\n",
    "LOGS_BASE = BASE_DIR / 'terminal_outputs'\n",
    "RESULTS_BASE = BASE_DIR / 'results'\n",
    "TRANSCRIPTS_BASE = BASE_DIR / 'transcripts'\n",
    "\n",
    "LANG_SETS = {\n",
    "    'english': 'English',\n",
    "    'spanish': 'Spanish',\n",
    "    'mandarin': 'Mandarin',\n",
    "}\n",
    "\n",
    "# Ensure subfolders exist (does not create config files)\n",
    "for key in LANG_SETS.keys():\n",
    "    (CONFIGS_BASE / key).mkdir(parents=True, exist_ok=True)\n",
    "    (LOGS_BASE / key).mkdir(parents=True, exist_ok=True)\n",
    "    (RESULTS_BASE / key).mkdir(parents=True, exist_ok=True)\n",
    "    (TRANSCRIPTS_BASE / key).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CONFIGS_BASE, LOGS_BASE, RESULTS_BASE, TRANSCRIPTS_BASE, LANG_SETS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229ea362",
   "metadata": {},
   "source": [
    "## 1) Config Generation \n",
    "\n",
    "Generates 34 YAML configurations for each language set.\n",
    "- All 5 agents share a per-config temperature drawn from U(0, 1.5).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77f14c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'english': 34, 'spanish': 34, 'mandarin': 34}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Income class probabilities (must sum to 1.0)\n",
    "INCOME_CLASS_PROBS = {\n",
    "    'high': 0.05,\n",
    "    'medium_high': 0.10,\n",
    "    'medium': 0.50,\n",
    "    'medium_low': 0.25,\n",
    "    'low': 0.10,\n",
    "}\n",
    "\n",
    "# Placeholder model list for participant agents — adjust as needed\n",
    "MODEL_LIST = [\n",
    "    \"gemini-2.5-pro\",\n",
    "    \"gemini-2.5-flash\",\n",
    "    \"gemini-2.5-flash-lite\",\n",
    "]\n",
    "\n",
    "def make_agents_with_models(temp: float, models: list[str]) -> list[dict]:\n",
    "    agents = []\n",
    "    for i in range(0, 5):  # 5 participant agents\n",
    "        agents.append({\n",
    "            'name': f'Agent_{i}',\n",
    "            'personality': 'You are an American college student',\n",
    "            'model': models[i],\n",
    "            'temperature': float(temp),\n",
    "            'memory_character_limit': 25000,\n",
    "            'reasoning_enabled': True,\n",
    "        })\n",
    "    return agents\n",
    "\n",
    "def build_config(lang: str, temp: float, seed_val: int, models: list[str]) -> dict:\n",
    "    return {\n",
    "        'language': lang,\n",
    "        'seed': int(seed_val),\n",
    "        'agents': make_agents_with_models(temp, models),\n",
    "        'utility_agent_model': \"gemini-2.5-flash-lite\",\n",
    "        'utility_agent_temperature': 0.0,\n",
    "        'phase2_rounds': 10,\n",
    "        'distribution_range_phase2': [2, 6],\n",
    "        'income_class_probabilities': INCOME_CLASS_PROBS,\n",
    "        'original_values_mode': { 'enabled': True },\n",
    "    }\n",
    "\n",
    "# Optional: set a global seed for reproducible generation (adjust or comment out)\n",
    "GLOBAL_SEED = 10000\n",
    "random.seed(GLOBAL_SEED)\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "\n",
    "def generate_aligned_configs(n: int = 34) -> dict[str, list[Path]]:\n",
    "    paths: dict[str, list[Path]] = {k: [] for k in LANG_SETS.keys()}\n",
    "    for idx in range(1, n + 1):\n",
    "        temp = random.uniform(0.0, 1.5)  # shared per condition across languages\n",
    "        seed_val = random.randint(0, 2**31 - 1)\n",
    "        models = [random.choice(MODEL_LIST) for _ in range(5)]  # shared agent models\n",
    "        for lang_key, lang_name in LANG_SETS.items():\n",
    "            cfg = build_config(lang=lang_name, temp=temp, seed_val=seed_val, models=models)\n",
    "            out_dir = (CONFIGS_BASE / lang_key)\n",
    "            out_dir.mkdir(parents=True, exist_ok=True)\n",
    "            fname = out_dir / f'hypothesis_6_{lang_key}_condition_{idx}_config.yaml'\n",
    "            with open(fname, 'w') as f:\n",
    "                yaml.safe_dump(cfg, f, sort_keys=False)\n",
    "            paths[lang_key].append(fname)\n",
    "    return paths\n",
    "\n",
    "# To generate aligned configs for all languages at once (writes 102 files total):\n",
    "#files_by_lang = generate_aligned_configs(n=34)\n",
    "#{k: len(v) for k, v in files_by_lang.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f3e163",
   "metadata": {},
   "source": [
    "## 2) Run Configs (Parallel per Language)\n",
    "\n",
    "Select subsets and run with per-language logs/results directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c99b0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_language_set(lang_key: str, include_indices=None, include_names=None, concurrency: int = 4, timeout_sec: int | None = None):\n",
    "    cfg_dir = CONFIGS_BASE / lang_key\n",
    "    logs_dir = LOGS_BASE / lang_key\n",
    "    results_dir = RESULTS_BASE / lang_key\n",
    "    configs = list_config_files(cfg_dir)\n",
    "\n",
    "    selected = select_configs(configs, include_indices=include_indices, include_names=include_names)\n",
    "    print(f'[{lang_key}] Found {len(configs)} configs; selected {len(selected)}')\n",
    "\n",
    "    run_results = run_configs_in_parallel(\n",
    "        selected,\n",
    "        concurrency=concurrency,\n",
    "        logs_dir=logs_dir,\n",
    "        results_dir=results_dir,\n",
    "        timeout_sec=timeout_sec,\n",
    "    )\n",
    "    ok = sum(1 for r in run_results if r.get('ok'))\n",
    "    print(f'[{lang_key}] Completed: {ok}/{len(run_results)} OK')\n",
    "    return run_results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20988bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running ALL ENGLISH conditions (34 configs) ===\n",
      "[english] Found 34 configs; selected 34\n",
      "[english] Completed: 31/34 OK\n",
      "Completed all ENGLISH conditions in 28887.2s\n",
      "\n",
      "=== Running ALL SPANISH conditions (34 configs) ===\n",
      "[spanish] Found 34 configs; selected 34\n",
      "[spanish] Completed: 34/34 OK\n",
      "Completed all SPANISH conditions in 41132.2s\n",
      "\n",
      "=== Running ALL MANDARIN conditions (34 configs) ===\n",
      "[mandarin] Found 34 configs; selected 34\n"
     ]
    }
   ],
   "source": [
    "# Run all conditions from each language sequentially by language\n",
    "# Starting with English, then Spanish, then Mandarin\n",
    "# Each language runs all its 34 conditions with concurrency=3\n",
    "all_run_results = []\n",
    "import time\n",
    "\n",
    "for lang_key in ['english', 'spanish', 'mandarin']:\n",
    "    print(f'\\n=== Running ALL {lang_key.upper()} conditions (34 configs) ===')\n",
    "    start_time = time.time()\n",
    "    run_results = run_language_set(\n",
    "        lang_key,\n",
    "        include_indices=list(range(1, 35)),  # All 34 conditions (1-34)\n",
    "        concurrency=2,        # Run 3 configs in parallel per language\n",
    "        timeout_sec=None\n",
    "    )\n",
    "    duration = time.time() - start_time\n",
    "    print(f'Completed all {lang_key.upper()} conditions in {duration:.1f}s')\n",
    "    all_run_results.extend(run_results)\n",
    "\n",
    "# Overall summary\n",
    "ok = sum(1 for r in all_run_results if r.get('ok'))\n",
    "total_configs = len(all_run_results)\n",
    "print(f'\\n=== OVERALL SUMMARY ===')\n",
    "print(f'Completed: {ok}/{total_configs} OK from {total_configs} configs (34 per language × 3 languages)')\n",
    "print(f'Expected total: {34 * 3} configs')\n",
    "#all_run_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfc722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_results = run_language_set(\n",
    "    'mandarin',\n",
    "    include_indices=list(range(14,35)),  # Only config 1\n",
    "    concurrency=4,\n",
    "    timeout_sec=None\n",
    ")\n",
    "\n",
    "\n",
    "# Display results\n",
    "for result in run_results:\n",
    "    if result.get('ok'):\n",
    "        print(f\"\\n✓ Success: {result['config']}\")\n",
    "        print(f\"  Result: {result.get('result_file')}\")\n",
    "    else:\n",
    "        print(f\"\\n✗ Failed: {result['config']}\")\n",
    "        print(f\"  Error: {result.get('error', 'Unknown error')}\")\n",
    "\n",
    "run_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61146eff",
   "metadata": {},
   "source": [
    "## 3) Analysis — Compare Outcomes Across Languages\n",
    "\n",
    "Build a 5×3 contingency table (rows=principle/disagreement categories; columns=English/Spanish/Mandarin)\n",
    "and run Fisher–Freeman–Halton exact test via R when available.\n",
    "Also compute Cramér's V with optional bootstrap CI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28d5237e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English counts: {'disagreement': 4, 'maximizing_average_floor_constraint': 30, 'maximizing_floor': 0, 'maximizing_average': 0, 'maximizing_average_range_constraint': 0}\n",
      "Spanish counts: {'maximizing_average_range_constraint': 2, 'disagreement': 15, 'maximizing_average_floor_constraint': 17, 'maximizing_floor': 0, 'maximizing_average': 0}\n",
      "Mandarin counts: {'maximizing_average_floor_constraint': 27, 'disagreement': 6, 'maximizing_floor': 1, 'maximizing_average': 0, 'maximizing_average_range_constraint': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 0,  0,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [30, 17, 27],\n",
       "        [ 0,  2,  0],\n",
       "        [ 4, 15,  6]]),\n",
       " ['maximizing_floor',\n",
       "  'maximizing_average',\n",
       "  'maximizing_average_floor_constraint',\n",
       "  'maximizing_average_range_constraint',\n",
       "  'disagreement'],\n",
       " ['english', 'spanish', 'mandarin'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CATEGORIES = [\n",
    "    'maximizing_floor',\n",
    "    'maximizing_average',\n",
    "    'maximizing_average_floor_constraint',\n",
    "    'maximizing_average_range_constraint',\n",
    "    'disagreement',\n",
    "]\n",
    "\n",
    "def categorize_result(result_path: Path) -> str:\n",
    "    try:\n",
    "        with open(result_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        gi = data.get('general_information', {})\n",
    "        consensus = gi.get('consensus_reached', False)\n",
    "        principle = gi.get('consensus_principle')\n",
    "        if consensus and principle in CATEGORIES:\n",
    "            return principle\n",
    "        return 'disagreement'\n",
    "    except Exception:\n",
    "        return 'disagreement'\n",
    "\n",
    "def count_by_language() -> dict[str, Counter]:\n",
    "    out: dict[str, Counter] = {}\n",
    "    for k in LANG_SETS.keys():\n",
    "        counts = Counter()\n",
    "        result_files = sorted((RESULTS_BASE / k).glob('*_results.json'))\n",
    "        for rp in result_files:\n",
    "            counts[categorize_result(rp)] += 1\n",
    "        for cat in CATEGORIES:\n",
    "            counts.setdefault(cat, 0)\n",
    "        out[k] = counts\n",
    "    return out\n",
    "\n",
    "lang_counts = count_by_language()\n",
    "for k, counts in lang_counts.items():\n",
    "    print(f'{k.capitalize()} counts:', dict(counts))\n",
    "\n",
    "# Build contingency table: rows=categories, cols=[English, Spanish, Mandarin]\n",
    "col_order = ['english', 'spanish', 'mandarin']\n",
    "contingency = np.vstack([[lang_counts[col][cat] for col in col_order] for cat in CATEGORIES])\n",
    "contingency, CATEGORIES, col_order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ca2a1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fisher–Freeman–Halton exact test p-value: 0.002187\n"
     ]
    }
   ],
   "source": [
    "def fisher_freeman_halton_pvalue_r(contingency: np.ndarray) -> float | None:\n",
    "    \"\"\"Run Fisher–Freeman–Halton test via R's fisher.test if available.\n",
    "    Returns p-value or None if Rscript not found or fails.\n",
    "    \"\"\"\n",
    "    if shutil.which('Rscript') is None:\n",
    "        return None\n",
    "    r_matrix = ','.join(str(int(x)) for x in contingency.flatten(order='C'))\n",
    "    nrow, ncol = contingency.shape\n",
    "    r_code = f\"\"\"m <- matrix(c({r_matrix}), nrow={nrow}, ncol={ncol}, byrow=TRUE);\n",
    "f <- tryCatch(fisher.test(m), error=function(e) NA);\n",
    "if (is.list(f)) {{ cat(f$p.value) }} else {{ cat('NA') }}\n",
    "\"\"\"\n",
    "    import subprocess\n",
    "    try:\n",
    "        out = subprocess.check_output(['Rscript', '-e', r_code], stderr=subprocess.STDOUT, text=True, timeout=30)\n",
    "        out = out.strip()\n",
    "        return float(out) if out and out != 'NA' else None\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "p_ffh = fisher_freeman_halton_pvalue_r(contingency)\n",
    "if p_ffh is None:\n",
    "    print('R not available; skipping Fisher–Freeman–Halton exact test')\n",
    "else:\n",
    "    print(f'Fisher–Freeman–Halton exact test p-value: {p_ffh:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "432c59f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cramér's V (standard): 0.2970\n",
      "Cramér's V (bias-corrected): 0.2443\n",
      "95% CI for bias-corrected V: [0.1176, 0.4076]\n"
     ]
    }
   ],
   "source": [
    "from hypothesis_testing.utils_hypothesis_testing import (\n",
    "    bias_corrected_cramers_v,\n",
    "    bootstrap_cramers_v,\n",
    "    cramers_v,\n",
    ")\n",
    "\n",
    "# Effect size summary\n",
    "if contingency.sum() > 0:\n",
    "    v_std = cramers_v(contingency, correction=False)\n",
    "    v_corr = bias_corrected_cramers_v(contingency, correction=False)\n",
    "    print(f\"Cramér's V (standard): {v_std:.4f}\")\n",
    "    print(f\"Cramér's V (bias-corrected): {v_corr:.4f}\")\n",
    "    boot_vs, ci_lo, ci_hi = bootstrap_cramers_v(\n",
    "        contingency,\n",
    "        n_bootstrap=2000,\n",
    "        confidence_level=0.95,\n",
    "        bias_corrected=True,\n",
    "        correction=False,\n",
    "        seed=123,\n",
    "    )\n",
    "    print(f\"95% CI for bias-corrected V: [{ci_lo:.4f}, {ci_hi:.4f}]\")\n",
    "else:\n",
    "    print('Insufficient data for effect size computation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63405ce",
   "metadata": {},
   "source": [
    "## Pairwise Comparisons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a0768d",
   "metadata": {},
   "source": [
    "### English vs Spanish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a59d4ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-contingency for english vs spanish:\n",
      "[[ 0  0]\n",
      " [ 0  0]\n",
      " [30 17]\n",
      " [ 0  2]\n",
      " [ 4 15]]\n",
      "Fisher–Freeman–Halton exact test p-value: 0.001617\n",
      "Cramér's V (bias-corrected): 0.3851\n"
     ]
    }
   ],
   "source": [
    "pair = ['english', 'spanish']\n",
    "sub_contingency = np.vstack([[lang_counts[col][cat] for col in pair] for cat in CATEGORIES])\n",
    "print(f'Sub-contingency for {pair[0]} vs {pair[1]}:')\n",
    "print(sub_contingency)\n",
    "p_ffh = fisher_freeman_halton_pvalue_r(sub_contingency)\n",
    "if p_ffh is None:\n",
    "    print('R not available; skipping Fisher–Freeman–Halton exact test')\n",
    "else:\n",
    "    print(f'Fisher–Freeman–Halton exact test p-value: {p_ffh:.6f}')\n",
    "v_corr = bias_corrected_cramers_v(sub_contingency)\n",
    "print(f\"Cramér's V (bias-corrected): {v_corr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6b0d00",
   "metadata": {},
   "source": [
    "### English vs Mandarin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c5f05ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-contingency for english vs mandarin:\n",
      "[[ 0  1]\n",
      " [ 0  0]\n",
      " [30 27]\n",
      " [ 0  0]\n",
      " [ 4  6]]\n",
      "Fisher–Freeman–Halton exact test p-value: 0.511789\n",
      "Cramér's V (bias-corrected): 0.0000\n"
     ]
    }
   ],
   "source": [
    "pair = ['english', 'mandarin']\n",
    "sub_contingency = np.vstack([[lang_counts[col][cat] for col in pair] for cat in CATEGORIES])\n",
    "print(f'Sub-contingency for {pair[0]} vs {pair[1]}:')\n",
    "print(sub_contingency)\n",
    "p_ffh = fisher_freeman_halton_pvalue_r(sub_contingency)\n",
    "if p_ffh is None:\n",
    "    print('R not available; skipping Fisher–Freeman–Halton exact test')\n",
    "else:\n",
    "    print(f'Fisher–Freeman–Halton exact test p-value: {p_ffh:.6f}')\n",
    "v_corr = bias_corrected_cramers_v(sub_contingency)\n",
    "print(f\"Cramér's V (bias-corrected): {v_corr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214c95f0",
   "metadata": {},
   "source": [
    "### Spanish vs Mandarin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba055f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-contingency for spanish vs mandarin:\n",
      "[[ 0  1]\n",
      " [ 0  0]\n",
      " [17 27]\n",
      " [ 2  0]\n",
      " [15  6]]\n",
      "Fisher–Freeman–Halton exact test p-value: 0.012048\n",
      "Cramér's V (bias-corrected): 0.3014\n"
     ]
    }
   ],
   "source": [
    "pair = ['spanish', 'mandarin']\n",
    "sub_contingency = np.vstack([[lang_counts[col][cat] for col in pair] for cat in CATEGORIES])\n",
    "print(f'Sub-contingency for {pair[0]} vs {pair[1]}:')\n",
    "print(sub_contingency)\n",
    "p_ffh = fisher_freeman_halton_pvalue_r(sub_contingency)\n",
    "if p_ffh is None:\n",
    "    print('R not available; skipping Fisher–Freeman–Halton exact test')\n",
    "else:\n",
    "    print(f'Fisher–Freeman–Halton exact test p-value: {p_ffh:.6f}')\n",
    "v_corr = bias_corrected_cramers_v(sub_contingency)\n",
    "print(f\"Cramér's V (bias-corrected): {v_corr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed79107d",
   "metadata": {},
   "source": [
    "## Floor Constraint Amount Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbf94d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language_key</th>\n",
       "      <th>language</th>\n",
       "      <th>result_file</th>\n",
       "      <th>floor_constraint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>english</td>\n",
       "      <td>English</td>\n",
       "      <td>hypothesis_6_english_condition_11_config_resul...</td>\n",
       "      <td>15000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>english</td>\n",
       "      <td>English</td>\n",
       "      <td>hypothesis_6_english_condition_12_config_resul...</td>\n",
       "      <td>13000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>english</td>\n",
       "      <td>English</td>\n",
       "      <td>hypothesis_6_english_condition_13_config_resul...</td>\n",
       "      <td>11000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>english</td>\n",
       "      <td>English</td>\n",
       "      <td>hypothesis_6_english_condition_14_config_resul...</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>english</td>\n",
       "      <td>English</td>\n",
       "      <td>hypothesis_6_english_condition_16_config_resul...</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>mandarin</td>\n",
       "      <td>Mandarin</td>\n",
       "      <td>hypothesis_6_mandarin_condition_5_config_resul...</td>\n",
       "      <td>12000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>mandarin</td>\n",
       "      <td>Mandarin</td>\n",
       "      <td>hypothesis_6_mandarin_condition_6_config_resul...</td>\n",
       "      <td>12000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>mandarin</td>\n",
       "      <td>Mandarin</td>\n",
       "      <td>hypothesis_6_mandarin_condition_7_config_resul...</td>\n",
       "      <td>12000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>mandarin</td>\n",
       "      <td>Mandarin</td>\n",
       "      <td>hypothesis_6_mandarin_condition_8_config_resul...</td>\n",
       "      <td>13000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>mandarin</td>\n",
       "      <td>Mandarin</td>\n",
       "      <td>hypothesis_6_mandarin_condition_9_config_resul...</td>\n",
       "      <td>40000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   language_key  language                                        result_file  \\\n",
       "0       english   English  hypothesis_6_english_condition_11_config_resul...   \n",
       "1       english   English  hypothesis_6_english_condition_12_config_resul...   \n",
       "2       english   English  hypothesis_6_english_condition_13_config_resul...   \n",
       "3       english   English  hypothesis_6_english_condition_14_config_resul...   \n",
       "4       english   English  hypothesis_6_english_condition_16_config_resul...   \n",
       "..          ...       ...                                                ...   \n",
       "71     mandarin  Mandarin  hypothesis_6_mandarin_condition_5_config_resul...   \n",
       "72     mandarin  Mandarin  hypothesis_6_mandarin_condition_6_config_resul...   \n",
       "73     mandarin  Mandarin  hypothesis_6_mandarin_condition_7_config_resul...   \n",
       "74     mandarin  Mandarin  hypothesis_6_mandarin_condition_8_config_resul...   \n",
       "75     mandarin  Mandarin  hypothesis_6_mandarin_condition_9_config_resul...   \n",
       "\n",
       "    floor_constraint  \n",
       "0              15000  \n",
       "1              13000  \n",
       "2              11000  \n",
       "3              20000  \n",
       "4              20000  \n",
       "..               ...  \n",
       "71             12000  \n",
       "72             12000  \n",
       "73             12000  \n",
       "74             13000  \n",
       "75             40000  \n",
       "\n",
       "[76 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-language floor constraint statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average</th>\n",
       "      <th>minimum</th>\n",
       "      <th>maximum</th>\n",
       "      <th>std_dev</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>language</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>14016.666667</td>\n",
       "      <td>1000</td>\n",
       "      <td>25000</td>\n",
       "      <td>4652.554607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mandarin</th>\n",
       "      <td>13074.074074</td>\n",
       "      <td>10000</td>\n",
       "      <td>40000</td>\n",
       "      <td>5599.972019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>16894.736842</td>\n",
       "      <td>9000</td>\n",
       "      <td>48000</td>\n",
       "      <td>10027.156110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               average  minimum  maximum       std_dev\n",
       "language                                              \n",
       "English   14016.666667     1000    25000   4652.554607\n",
       "Mandarin  13074.074074    10000    40000   5599.972019\n",
       "Spanish   16894.736842     9000    48000  10027.156110"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall floor constraint statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average</th>\n",
       "      <th>minimum</th>\n",
       "      <th>maximum</th>\n",
       "      <th>std_dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>all_languages</th>\n",
       "      <td>14401.315789</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>6755.74804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    average  minimum  maximum     std_dev\n",
       "all_languages  14401.315789   1000.0  48000.0  6755.74804"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "def extract_floor_constraints() -> pd.DataFrame:\n",
    "    rows: list[dict[str, object]] = []\n",
    "    for lang_key, lang_label in LANG_SETS.items():\n",
    "        result_files = sorted((RESULTS_BASE / lang_key).glob('*_results.json'))\n",
    "        for result_path in result_files:\n",
    "            with open(result_path, 'r') as fp:\n",
    "                data = json.load(fp)\n",
    "            vote_rounds = data.get('voting_history', {}).get('vote_rounds', [])\n",
    "            agreed_constraint = None\n",
    "            for vote_round in reversed(vote_rounds):\n",
    "                if vote_round.get('agreed_constraint') is not None:\n",
    "                    agreed_constraint = vote_round['agreed_constraint']\n",
    "                    break\n",
    "            if agreed_constraint is not None:\n",
    "                rows.append({\n",
    "                    'language_key': lang_key,\n",
    "                    'language': lang_label,\n",
    "                    'result_file': result_path.name,\n",
    "                    'floor_constraint': agreed_constraint,\n",
    "                })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "floor_constraints_df = extract_floor_constraints()\n",
    "display(floor_constraints_df)\n",
    "\n",
    "if not floor_constraints_df.empty:\n",
    "    per_language_stats = floor_constraints_df.groupby('language')['floor_constraint'].agg(\n",
    "        average='mean',\n",
    "        minimum='min',\n",
    "        maximum='max',\n",
    "        std_dev='std',\n",
    "    )\n",
    "    overall_stats = floor_constraints_df['floor_constraint'].agg(['mean', 'min', 'max', 'std']).rename({\n",
    "        'mean': 'average',\n",
    "        'min': 'minimum',\n",
    "        'max': 'maximum',\n",
    "        'std': 'std_dev',\n",
    "    })\n",
    "    overall_stats_df = pd.DataFrame([overall_stats], index=['all_languages'])\n",
    "    print('\\nPer-language floor constraint statistics:')\n",
    "    display(per_language_stats)\n",
    "    print('\\nOverall floor constraint statistics:')\n",
    "    display(overall_stats_df)\n",
    "else:\n",
    "    print('No floor constraint values found across runs.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
